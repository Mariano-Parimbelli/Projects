{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6e1dd91",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d282ea",
   "metadata": {},
   "outputs": [],
   "source": [
    " #cadena=input()\n",
    "cadena=\"quisiera saber las noticias de hoy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e73cbcfb",
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def correcciones(cadena):\n",
    "   \n",
    "    import spacy\n",
    "    nlp=spacy.load('es_core_news_lg')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    import re\n",
    "    from spacy.matcher import Matcher\n",
    "    palabra_frecuencia = {}\n",
    "    global pregunta_listata\n",
    "    with open(r\"Palabra_Frecuencia.txt\",encoding='utf-8') as f:\n",
    "        for linea in f:\n",
    "            (key, val) = linea.split('#')\n",
    "            palabra_frecuencia[key] = int(val)\n",
    "\n",
    "    def frecuencia(p):\n",
    "        try:\n",
    "            return palabra_frecuencia[p]\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "\n",
    "\n",
    "    def remove_repeated_characters(tokens,s=100): \n",
    "       repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)') \n",
    "       match_substitution = r'\\1\\2\\3'\n",
    "\n",
    "       def replace(old_word):\n",
    "          if frecuencia(old_word)>s:\n",
    "             return old_word\n",
    "          new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "          return replace(new_word) if new_word != old_word else new_word\n",
    "\n",
    "       correct_tokens = [replace(word) for word in tokens]\n",
    "       return correct_tokens\n",
    "\n",
    "\n",
    "    #convierto en cadena la lista\n",
    "    cadena=\"\".join(cadena)\n",
    "    doc = nlp(cadena)\n",
    "    tokens_str=[str(x) for x in doc]\n",
    "    cadena=remove_repeated_characters(tokens_str)\n",
    "\n",
    "\n",
    "    def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    WORDS = palabra_frecuencia\n",
    "\n",
    "    def corregir(oracion):\n",
    "        oracion_corregida=''\n",
    "        for palabra in re.findall(r'\\w+',oracion):\n",
    "            oracion_corregida += correction(palabra)[0] + ' '\n",
    "        return oracion_corregida\n",
    "\n",
    "    def correction(word): \n",
    "        \"Most probable spelling correction for word.\"\n",
    "        return [max(candidates(word), key=frecuencia)]\n",
    "\n",
    "    def candidates(word): \n",
    "        \"Generate possible spelling corrections for word.\"\n",
    "        return (known([word]) or \n",
    "                known(edits1(word)) or \n",
    "                #known(edits2(word)) or \n",
    "                [word])\n",
    "\n",
    "    def known(words): \n",
    "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "        return set(w for w in words if w in WORDS)\n",
    "\n",
    "    def edits1(word):\n",
    "        \"All edits that are one edit away from `word`.\"\n",
    "        letters    = 'abcdefghijklmnÃ±opqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "\n",
    "\n",
    "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "\n",
    "\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "\n",
    "\n",
    "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "\n",
    "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(word): \n",
    "        \"All edits that are two edits away from `word`.\"\n",
    "        return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "    corregida=corregir(str(cadena))\n",
    "\n",
    "     #===ELIMINAR STOP WORDS===\n",
    "\n",
    "    def stopWords(corpus):\n",
    "        global oraciones_filtradas\n",
    "        f = open('stopwords.txt', 'r')\n",
    "        stopwords = f.read().split('\\n')\n",
    "        f.close()\n",
    "        \" \".join(stopwords)\n",
    "        oraciones=pd.Series(corpus)\n",
    "\n",
    "        oraciones_filtradas = [\" \".join([\n",
    "                                  palabra for palabra in oracion.split()\n",
    "                                  if palabra not in stopwords])\n",
    "                                  for oracion in oraciones]\n",
    "    stopWords(corregida)\n",
    "\n",
    "    #pregunta_lista = \" \".join(oraciones_filtradas)\n",
    "\n",
    "    #===STEMMATIZAR===\n",
    "    from nltk.stem import SnowballStemmer\n",
    "\n",
    "    spanish_stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "    oraciones_filtradas_lematizadas=[]\n",
    "    for oracion in oraciones_filtradas:\n",
    "        oracion_lematizada = []\n",
    "        for palabra in oracion.split():\n",
    "            doc=nlp(palabra)\n",
    "            oracion_lematizada.append(doc[0].lemma_)\n",
    "        oraciones_filtradas_lematizadas.append(\" \".join(oracion_lematizada))\n",
    "\n",
    "    global pregunta_lista\n",
    "    oraciones_filtradas_lematizadas_stemm=[]\n",
    "\n",
    "    for oracion in oraciones_filtradas_lematizadas:\n",
    "        oracion_stemm = []\n",
    "        for palabra in oracion.split():\n",
    "            oracion_stemm.append(spanish_stemmer.stem(palabra))\n",
    "        oraciones_filtradas_lematizadas_stemm.append(\" \".join(oracion_stemm))\n",
    "\n",
    "    pregunta_lista=oraciones_filtradas_lematizadas_stemm\n",
    "correcciones(cadena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ca5a6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quer sab notici hoy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pregunta_lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffa038d9",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lobo\\anaconda3\\envs\\IA\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\lobo\\anaconda3\\envs\\IA\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def prediccion(corpus):\n",
    "    import pickle\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    global result\n",
    "    global probabilidad\n",
    "    \n",
    "    loaded_vec = CountVectorizer(vocabulary=pickle.load(open(\"prediccion/estructure.pkl\", \"rb\"))) \n",
    "    new_input=loaded_vec.transform(corpus)\n",
    "    new_input=pd.DataFrame(new_input.toarray()) \n",
    "\n",
    "\n",
    "    clf_input=pickle.load(open(\"prediccion/modelo.sav\", \"rb\"))\n",
    "    probabilidad=sorted(list(clf_input.predict_proba(new_input)[0]))[-1]\n",
    "    predict_input=clf_input.predict(new_input)\n",
    "\n",
    " \n",
    "    for index,value in enumerate(predict_input):\n",
    "        result=[]\n",
    "        if float(probabilidad) >= 0.4:\n",
    "            result.append(predict_input[index])\n",
    "\n",
    "    \n",
    "    result=np.unique(result)\n",
    "    result=result.tolist() \n",
    "prediccion(pregunta_lista) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47f593e3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noticias']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e7b219b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245996989680577"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3c036",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a69613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55390b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
